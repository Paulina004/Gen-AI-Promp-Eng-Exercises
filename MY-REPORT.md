![GenI-Banner](https://github.com/genilab-fau/genial-fau.github.io/blob/8f1a2d3523f879e1082918c7bba19553cb6e7212/images/geni-lab-banner.png?raw=true)

# {title of your research project}

1-liner description of your project

<!-- WHEN APPLICABLE, REMOVE THE COMMENT MARK AND COMPLETE
This is a response to the Assignment part of the COURSE.
-->

* Authors: [Paulina DeVito](https://www.linkedin.com/in/paulina-devito-fau/)
* Academic Supervisor: [Dr. Fernando Koch](http://www.fernandokoch.me)

  
# Research Question 

How do different prompt engineering techniques impact the effectiveness of LLMs in performing requirement analysis across various models and parameters?

## Arguments

#### What Is Already Known About This Topic

* you could do {something} to achieve {some result}
* the challenges of {doing something}
* the possibility of {doing something else}
* ...
* Gen AI in the SDLC process...
* ...
* You could use prompt engineering techniques to optimize LLM performance for specific tasks.
* The challenges of selecting the right prompt engineering strategy depend on factors like task complexity and model complexity.
* The possibility of combining multiple prompt engineering techniques to improve LLM output consistency and accuracy. ???
* Different LLM architectures and configurations may respond differently to prompt variations, influencing performance.
* Requirement analysis in software development traditionally relies on structured methodologies, but LLMs offer a new, flexible approach.

#### What This Research Is Exploring

<!-- Free-format; use the topics that are applicable to your exploration  -->

* This study employs a structured comparison of multiple prompt engineering techniques, such as few-shot prompting and meta-prompting.
* A ranking framework was used to systematically evaluate how LLMs perform requirement analysis for a specific product idea--in our case, a hypothetical study Discord-based chatbot system.
* The authors of this study are exploring the impact of model selection, parameter tuning, and prompt engineering strategies on the accuracy and reliability of requirement analysis outputs.
* This study highlights how certain models alongside advanced prompting techniques like prompt automation (***) can contribute to better requirement elicitation in an SDLC process. 

#### Implications for Practice

<!-- Free-format; use the topics that are applicable to your exploration  -->

* it will be easier to {do something}
* it will optimize {some process}
* we will better understand {some process}
* ...

# Research Method

We are conducting an experimental study where different prompt engineering techniques are applied to LLMs tasked with performing requirement analysis for a study chatbot system. Key aspects of the research method include:

    Testing a variety of LLMs (e.g., GPT, Claude, Gemini, Mistral) across different configurations.
    Evaluating prompt engineering variations, including zero-shot, few-shot, chain-of-thought, retrieval-augmented generation, and Reflexion.
    Measuring performance (see below).
    Analyzing the impact of model parameters (e.g., temperature, max tokens) on output quality.
    Comparing human-expert-reviewed results to assess effectiveness.

## Ranking

### Considerations
The following dimensions of the responses from the generative AI models were considered: 
1. **Completeness:** Does the response cover all key aspects of requirement analysis (functional, non-functional, constraints, assumptions, etc.)?
2. **Accuracy & Relevance:** Is the information provided correct and appropriate for the use case and original prompt?
3. **Clarity & Structure:** Is the response well-organized, easy to follow, and structured logically?
4. **Depth & Insight:** Does the response provide in-depth analysis, considering edge cases and complexities of the use case?
5. **Consistency & Coherence:** Does the response maintain a logical flow without contradictions?
GPT 4o was asked to rank the responses, and the rankings from GPT were human analyzed by the main author of this study (Paulina DeVito).

### Scale
Scores were given to each response as follows:
* **0 – Unacceptable:** The response is irrelevant, incoherent, or completely incorrect.
* **1 – Poor:** Some relevant points are mentioned, but the response is incomplete, lacks structure, or contains major inaccuracies.
* **2 – Fair:** The response has some useful insights but is either missing key details or has moderate inaccuracies.
* **3 – Good:** The response is mostly complete, well-structured, and mostly accurate, but lacks depth or minor details.
* **4 – Excellent:** The response is thorough, well-organized, insightful, and highly relevant, with strong attention to detail.

# Results

Describe the results achieved through your research process.

# Further Research

* Try more model parameter configurations.
* Test more models, like GPT.
*

